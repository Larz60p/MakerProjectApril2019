{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# April 2019 Python Users Group - Manchester, New Hampshire, USA\n",
    "\n",
    "Author: Larry M. (Larz60+)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "##  MIT License\n",
    "\n",
    "Copyright (c) 2018 Larz60+\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "##  Introduction\n",
    "\n",
    "\n",
    "Originally, I wanted to present examples of three types of scrapers, one using lxml, another using BeautifulSoup and finally one using Selenium. I don't think I could present all three in one evening and still have good coverage of each section, so I'll leave lxml for another presentation. Instead, I'll be scraping a site that requires both selenium and BeautifulSoup. A good candidate for this is the Connecticut Secretary of State Business Entity Search site. \n",
    "\n",
    "The modules needed to accomplish this are:\n",
    "\n",
    "### Set-up\n",
    "\n",
    "* CreateCityFile.py - Create city dictionary, saved as json file by scraping 'https://ctstatelibrary.org/cttowns/counties' using selenium and BeautifulSoup. Will\n",
    "  explain how to capture XPath using Inspect Element. Page will be captured using Selenium which will expand the java script. The parsing will be done using\n",
    "  Beautifulsoup.\n",
    "\n",
    "* CreateDict.py - Module to create class for creating dictionaries, add nodes, and add cells. (reusable)\n",
    "\n",
    "* Copy PrettifyPage.py from other application- used to create formatted pretty pages (uses BeautifulSoup4 prettify, but modifies spacing to make it easier to read.\n",
    "\n",
    "* BusinessPaths.py - Create a Paths class, that contains entries for and creates directories (if necessary) for all modules in package. Included are all file\n",
    "  names and locations. Having a file like this saves a lot of time trying to chase down where a particular file is located. It is used by all modules, and\n",
    "  because of this, any file name and/or directory can be modified, and immediately propagated to all modules in package. Uses pathlib.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "* PreviewSearchPage.py - Runs a test using Seleium to see if there's a hidden API somewhere, which would greatly simplify scraping.\n",
    "\n",
    "### Scrape\n",
    "\n",
    "* ScrapeConnecticut.py - Capture all summary pages searching by City, which is the best method for getting total coverage of all businesses.\n",
    "  Following done using concurrentfutures.ProcessPoolExecutor. These files are stored in ./data/html and named City_pagex.html example: Andover_page12.html\n",
    "\n",
    "* ExtractBusinessEntities.py - Extracts business information from Summary files, and saves as preliminary information in ./data/json/CompanyMaster.json includes\n",
    "  basic company information as well as URL of Detail information, and associated filename.\n",
    "\n",
    "* AddEntityDetail.py - Using information gathered in ExtractBusinessEntities.py, downloads all detail files, and adds detail information on each business to the\n",
    "  ./data/json/CompanyMaster.json file.\n",
    "\n",
    "### Process\n",
    "\n",
    "* CreateDatabase.py - Creates an sqlite3 database located at ./data/database/CompanyMaster.db. Populated from CompanyMain.json, CompanyDetail.json,\n",
    "  CompanyPrincipals.json, CompanyAgents.json and CompanyFilings.json whics are generated from CompanyMaster.json. Calls CreateTables.py\n",
    "\n",
    "* CreateTables.py - Creates database and all tables in Company sqlite3 database\n",
    "  \n",
    "* Future - Create GUI to access scraped data. (Might actually have something by meeting time).\n",
    "\n",
    "* The website used to scrape above business data is in process of change. Will need to rewrite all modules in Scrape section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## BusinessPaths.py\n",
    "\n",
    "This module creates all data directories (only if they don't already exist), filenames and paths (once in a while I will create tmp files on the fly, but will\n",
    "almost always use paths from this module). I have been doing this for some time now and have never regretted it. Advantages:\n",
    "\n",
    "* Can add new files, which instantly become available to every program in the package.\n",
    "* Never have to wonder where a file is stored, as only it's name is necessary for access.\n",
    "* Entire directory can be renamed with 100 % transparency.\n",
    "* Most of the directory names become standard to all the code that I write, and users know where to look for a certain type of file.\n",
    "\n",
    "Probably many other advantages that I can't think of at the moment.\n",
    "\n",
    "The very first statement in each and every one of these files is an anchor that establishes the relationship of all files and directorys, and the modules themselves.\n",
    "The anchor is based of the __file__ variables associated with a module, and is written as:\n",
    "os.chdir( os.path.abspath( os.path.dirname( __file__ )))\n",
    "\n",
    "Initializes all paths, URLs and files (other that temporary files). It is imported into all modules\n",
    "\n",
    "Create .data directory and all sub directories if not already created, does not destroy existing directories or data. Safe to run.\n",
    "\n",
    "Script should be run by itself once on a new system install.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BusinessPaths.py\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "class BusinessPaths:\n",
    "    def __init__(self):\n",
    "        os.chdir(os.path.abspath(os.path.dirname(__file__)))\n",
    "        self.homepath = Path('.')\n",
    "        self.rootpath = self.homepath / '..'\n",
    "\n",
    "        self.datapath = self.rootpath / 'data'\n",
    "        self.datapath.mkdir(exist_ok=True)\n",
    "\n",
    "        self.dbpath = self.datapath / 'database'\n",
    "        self.dbpath.mkdir(exist_ok=True)\n",
    "\n",
    "        self.htmlpath = self.datapath / 'html'\n",
    "        self.htmlpath.mkdir(exist_ok=True)\n",
    "\n",
    "        self.idpath = self.datapath / 'Idfiles'\n",
    "        self.idpath.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.jsonpath = self.datapath / 'json'\n",
    "        self.jsonpath.mkdir(exist_ok=True)\n",
    "\n",
    "        self.prettypath = self.datapath / 'pretty'\n",
    "        self.prettypath.mkdir(exist_ok=True)\n",
    "\n",
    "        self.textpath = self.datapath / 'text'\n",
    "        self.textpath.mkdir(exist_ok=True)\n",
    "\n",
    "        self.tmppath = self.datapath / 'tmp'\n",
    "        self.tmppath.mkdir(exist_ok=True)\n",
    "\n",
    "        self.base_url = 'http://searchctbusiness.ctdata.org/'\n",
    "\n",
    "        self.cities_json = self.jsonpath / 'cities.json'\n",
    "        self.city_list_url = 'https://ctstatelibrary.org/cttowns/counties'\n",
    "        self.raw_city_file = self.tmppath / 'raw_city.html'\n",
    "        # self.cities_text = self.textpath / 'cities.txt'\n",
    "\n",
    "        self.company_master_json = self.jsonpath / 'CompanyMaster.json'\n",
    "\n",
    "        self.CompanyMasterDb = self.dbpath / 'CompanyMaster.db'\n",
    "\n",
    "        self.company_main = self.jsonpath / 'CompanyMain.json'\n",
    "        self.company_detail = self.jsonpath / 'CompanyDetail.json'\n",
    "        self.company_principals = self.jsonpath / 'CompanyPrincipals.json'\n",
    "        self.company_agents = self.jsonpath / 'CompanyAgents.json'\n",
    "        self.company_filings = self.jsonpath / 'CompanyFilings.json'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BusinessPaths()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## CreateDict.py\n",
    "\n",
    "I got tired of trying to remember exactly how (and where) to add new dictionaries, nodes or cells, so I wrote this module.\n",
    "\n",
    "### new_dict(self, dictname)\n",
    "* Create a new dictionary instance named dictname. I usually manually create the dictionary in the __init__ method of class, but this can be used as an alternative\n",
    "\n",
    "### add_node(self, parent, nodename)\n",
    "* Adds a new nested node to parent node, named nodename\n",
    "\n",
    "### Add cell(self, nodename, cellname, value)\n",
    "* Adds a cell to node nodename, named cellname, with and assigns value to cell.\n",
    "\n",
    "### display_dict(self, dictname, level=0)\n",
    "* Display dictionary (or node of dictionary) in a nicely formatted and properly indented manner.\n",
    "* level is indent level, and never supplied by caller, it's used for determining indent level for recurrsive calls.\n",
    "* testit is a demo of usage.\n",
    "  - Results of running testit:\n",
    "  - CityList Dictionary\n",
    "\n",
    "    Boston\n",
    "\n",
    "        Resturants\n",
    "\n",
    "            Spoke Wine Bar\n",
    "                Addr1: 02144\n",
    "                City: Sommerville\n",
    "                Phone: 617-718-9463\n",
    "\n",
    "            Highland Kitchen\n",
    "                Addr1: 150 Highland Ave\n",
    "                City: Sommerville\n",
    "                ZipCode: 02144\n",
    "                Phone: 617-625-1131\n",
    "\n",
    "    raw data: {'Boston': {'Resturants': {'Spoke Wine Bar': {'Addr1': '02144', 'City': 'Sommerville', 'Phone': '617-718-9463'}, 'Highland Kitchen': {'Addr1': '150 Highland Ave', 'City': 'Sommerville', 'ZipCode': '02144', 'Phone': '617-625-1131'}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateDict.py\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class CreateDict:\n",
    "    def __init__(self):\n",
    "        os.chdir(os.path.abspath(os.path.dirname(__file__)))\n",
    "\n",
    "    def new_dict(self, dictname):\n",
    "        setattr(self, dictname, {})\n",
    "\n",
    "    def add_node(self, parent, nodename):\n",
    "        node = parent[nodename] = {}\n",
    "        return node\n",
    "\n",
    "    def add_cell(self, nodename, cellname, value):\n",
    "        cell =  nodename[cellname] = value\n",
    "        return cell\n",
    "\n",
    "    def display_dict(self, dictname, level=0):\n",
    "        indent = \" \" * (4 * level)\n",
    "        for key, value in dictname.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f'\\n{indent}{key}')\n",
    "                level += 1\n",
    "                self.display_dict(value, level)\n",
    "            else:\n",
    "                print(f'{indent}{key}: {value}')\n",
    "            if level > 0:\n",
    "                level -= 1\n",
    "\n",
    "\n",
    "def testit():\n",
    "    # instantiate class\n",
    "    cd = CreateDict()\n",
    "\n",
    "    # create new dictionary named CityList\n",
    "    cd.new_dict('CityList')\n",
    "\n",
    "    # add node Boston\n",
    "    boston = cd.add_node(cd.CityList, 'Boston')\n",
    "    # add sub node Resturants\n",
    "    bos_resturants = cd.add_node(boston, 'Resturants')\n",
    "\n",
    "    # Add subnode 'Spoke Wine Bar' to parent bos_resturants\n",
    "    spoke = cd.add_node(bos_resturants, 'Spoke Wine Bar')\n",
    "    cd.add_cell(spoke, 'Addr1', '89 Holland St')\n",
    "    cd.add_cell(spoke, 'City', 'Sommerville')\n",
    "    cd.add_cell(spoke, 'Addr1', '02144')\n",
    "    cd.add_cell(spoke, 'Phone', '617-718-9463')\n",
    "\n",
    "    # Add subnode 'Highland Kitchen' to parent bos_resturants\n",
    "    highland = cd.add_node(bos_resturants, 'Highland Kitchen')\n",
    "    cd.add_cell(highland, 'Addr1', '150 Highland Ave')\n",
    "    cd.add_cell(highland, 'City', 'Sommerville')\n",
    "    cd.add_cell(highland, 'ZipCode', '02144')\n",
    "    cd.add_cell(highland, 'Phone', '617-625-1131')\n",
    "\n",
    "    # display dictionary\n",
    "    print(f'\\nCityList Dictionary')\n",
    "    cd.display_dict(cd.CityList)\n",
    "    print(f'\\nraw data: {cd.CityList}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    testit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## CreateCityFile.py\n",
    "\n",
    "This script creates a json file that will be used to control the order and scope of page retrevial.\n",
    "\n",
    "This method was chosen over search by name Alphabetically because:\n",
    "* The Connecticut search by Name requires a minimum or two letters\n",
    "* It doesn't care where within the name those two letters occur \n",
    "* This means a search for 'AA' will find\n",
    "  'VEREIT AA STRATFORD CT, LLC'\n",
    "  'AA CLEANING LLC'\n",
    "  but not 'AAA AFFORDABLE GLASS, INC.'\n",
    "* because of this, the number of pages needed for full coverage\n",
    "  is massive, and probably impossible because of practability.\n",
    "  \n",
    "Coverage by city has no page limit (at time of writing)\n",
    "* example a city search on bridgeport brings up 741 pages.\n",
    "* This allows for full coverage of all companies.\n",
    "\n",
    "The URL I chose to get city information is https://ctstatelibrary.org/cttowns/counties. \n",
    "This page contains more information than is needed for this package.\n",
    "Nevertheless, I'll save everything ... It may with  high probability, prove useful in some future package.\n",
    "\n",
    "To start, capture the city page using selenium. This will capture the javascript city table in the browser page.\n",
    "* Show Inispect Element to get css select for header\n",
    "* Show how to determine table layout using print statements\n",
    "* Then uncomment code and run again, showing the dictionary that's written to the json file (data/json/cities.json).\n",
    "* Also demonstrate use of pythomn json formatter to create text file of dictionary\n",
    "\n",
    "Each city has the following format:\n",
    "* \"Andover\": {\n",
    "  - \"Town name\": \"Andover\",\n",
    "  - \"County\": \"Tolland\",\n",
    "  - \"Year Established\": \"1848\",\n",
    "  - \"Parent Town\": \"Coventry, Hebron\",\n",
    "  - \"History of incorporation\": \"May 18, 1848; taken from Hebron and Coventry\",\n",
    "  - \"ContiguousCityName\": \"Andover\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateCityFile.py\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import BusinessPaths\n",
    "import time\n",
    "import PrettifyPage\n",
    "import CreateDict\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "class CreateCityFile:\n",
    "    def __init__(self):\n",
    "        self.bpath = BusinessPaths.BusinessPaths()\n",
    "        self.pp = PrettifyPage.PrettifyPage()\n",
    "        self.cd = CreateDict.CreateDict()\n",
    "\n",
    "        self.header = []\n",
    "        self.city_info = {}\n",
    "\n",
    "        self.get_city_info()\n",
    "\n",
    "    def start_browser(self):\n",
    "        caps = webdriver.DesiredCapabilities().FIREFOX\n",
    "        caps[\"marionette\"] = True\n",
    "        self.browser = webdriver.Firefox(capabilities=caps)\n",
    "\n",
    "    def stop_browser(self):\n",
    "        self.browser.close()\n",
    "\n",
    "    def get_city_info(self):\n",
    "        if not self.bpath.cities_json.exists():\n",
    "            if self.bpath.raw_city_file.exists():\n",
    "                with self.bpath.raw_city_file.open() as fp:\n",
    "                    page = fp.read()\n",
    "                soup = BeautifulSoup(page, \"lxml\")\n",
    "            else:\n",
    "                self.start_browser()\n",
    "                self.browser.get(self.bpath.city_list_url)\n",
    "                time.sleep(2)\n",
    "                page = self.browser.page_source\n",
    "                # save working copy and pretty copy for analysis in temp\n",
    "                with self.bpath.raw_city_file.open('w') as fp:\n",
    "                    fp.write(page)\n",
    "                soup = BeautifulSoup(page, \"lxml\")\n",
    "                prettyfile = self.bpath.prettypath / 'raw_city_file_pretty.html'\n",
    "                with prettyfile.open('w') as fp:\n",
    "                    fp.write(f'{self.pp.prettify(soup, 2)}')\n",
    "                self.stop_browser()\n",
    "            table = soup.find('table', {'summary': 'This table displays Connecticut towns and the year of their establishment.'})\n",
    "            trs = table.tbody.find_all('tr')\n",
    "\n",
    "            # Create Node to separate Connecticut - May contain multiple states later\n",
    "            masternode = self.cd.add_node(self.city_info, 'Connecticut')\n",
    "            \n",
    "            citynode = None\n",
    "            contigname = 'Unspecified'\n",
    "            for n, tr in enumerate(trs):\n",
    "                if n == 0:\n",
    "                    self.header = []\n",
    "                    for td in self.get_td(tr):                            \n",
    "                        self.header.append(td.p.b.i.text.strip())\n",
    "                    self.cd.add_cell(masternode, 'Header', self.header)\n",
    "                else:\n",
    "                    for n1, td in enumerate(self.get_td(tr)):\n",
    "                        # print(f'==================================== tr {n}, td: {n1} ====================================')\n",
    "                        # print(f'{self.pp.prettify(td, 2)}')\n",
    "                        if n1 == 0:\n",
    "                            citynode = self.cd.add_node(masternode, f'{td.p.text.strip()}')\n",
    "                        value = td.p\n",
    "                        if td.p is None:\n",
    "                            value = 'Unspecified'\n",
    "                        else:\n",
    "                            value = td.p.text.strip()\n",
    "                            if value == '—-':\n",
    "                                value ='No parent town'\n",
    "                        self.cd.add_cell(citynode, self.header[n1], value.strip())\n",
    "                        if self.header[n1] == 'Town name':\n",
    "                            contigname = value.strip().replace(' ', '')\n",
    "                    self.cd.add_cell(citynode, 'ContiguousCityName', contigname)\n",
    "            self.cd.display_dict(self.city_info)\n",
    "\n",
    "            # Create json file\n",
    "            with self.bpath.cities_json.open('w') as fp:\n",
    "                json.dump(self.city_info, fp)\n",
    "\n",
    "\n",
    "    def get_td(self, tr):\n",
    "        tds = tr.find_all('td')\n",
    "        for td in tds:\n",
    "            yield td\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CreateCityFile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1450\"\n",
       "            height=\"900\"\n",
       "            src=\"http://searchctbusiness.ctdata.org/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1f5c9e26a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='http://searchctbusiness.ctdata.org/', width=1450, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## PreviewSearchPage.py\n",
    "\n",
    "Take a look at the search page above. At this point in time, it's the go to page for any Business Entity Search for Connecticut.\n",
    "\n",
    "Connecticut is considering publishing an API for Business Entity search, but does not yet do that.\n",
    "\n",
    "I have found that many (if not most) searches do indeed have an underlying API. It only has to be discovered.\n",
    "\n",
    "The purpose of this script is to expose that API if indeed it does exist.\n",
    "The method used to do this is to scrape a page using Selenium, using a city that is known to contain several pages of data.\n",
    "\n",
    "What is garnered here?\n",
    "\n",
    "* URL for Main page, just as a warm and fuzzy (makes sure I am using proper and full URL)\n",
    "* URL for initial search page for selected city.\n",
    "* URL for 2nd page for selected city\n",
    "* URL for Detail information of just one company.\n",
    "\n",
    "If there is an API, it should be exposed by step 3.\n",
    "But need to see step 2 results to see if first page needs to be treated in a special way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreviewSerchPage.py\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import BusinessPaths\n",
    "import time\n",
    "import PrettifyPage\n",
    "import GetPage\n",
    "import CreateDict\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "class PreviewSearchPage:\n",
    "    def __init__(self):\n",
    "        self.bpath = BusinessPaths.BusinessPaths()\n",
    "        self.pp = PrettifyPage.PrettifyPage()\n",
    "        self.cd = CreateDict.CreateDict()\n",
    "        self.gp = GetPage.GetPage()\n",
    "        self.getpage = self.gp.get_page\n",
    "\n",
    "        self.analyze_page()\n",
    "\n",
    "    def start_browser(self):\n",
    "        caps = webdriver.DesiredCapabilities().FIREFOX\n",
    "        caps[\"marionette\"] = True\n",
    "        self.browser = webdriver.Firefox(capabilities=caps)\n",
    "\n",
    "    def stop_browser(self):\n",
    "        self.browser.close()\n",
    "\n",
    "    def save_page(self, filename):\n",
    "        soup = BeautifulSoup(self.browser.page_source, \"lxml\")\n",
    "        with filename.open('w') as fp:\n",
    "            fp.write(self.pp.prettify(soup, 2))\n",
    "    \n",
    "    def analyze_page(self):\n",
    "        self.start_browser()\n",
    "        self.get_search_page('Andover')\n",
    "        self.stop_browser()\n",
    "    \n",
    "    def get_search_page(self, searchitem):\n",
    "        # pick city with multiple pages\n",
    "        url = 'http://searchctbusiness.ctdata.org'\n",
    "        \n",
    "        # Get the main page\n",
    "        self.browser.get(url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Even though we already know what the URL for first page is, display captured URL\n",
    "        # in order to avoid any surprises.\n",
    "        print(f'Main Page URL: {self.browser.current_url}')\n",
    "\n",
    "        # Find the 'Business City' option and select it\n",
    "        self.browser.find_element(By.XPATH, '/html/body/div[2]/div[4]/div/form/div/div/span[1]/select/option[3]').click()\n",
    "        \n",
    "        # Finde the search box, make sure it's cleared, and insert our test page 'Andover'\n",
    "        searchbox = self.browser.find_element(By.XPATH, '//*[@id=\"query\"]')\n",
    "        searchbox.clear()\n",
    "        searchbox.send_keys(searchitem)\n",
    "        \n",
    "        # Find the select button and click it to go to First page\n",
    "        self.browser.find_element(By.XPATH, '/html/body/div[2]/div[4]/div/form/div/div/span[3]/button').click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Display first results page URL\n",
    "        print(f'Results Page 1 URL: {self.browser.current_url}')\n",
    "        \n",
    "        # find total number of pages, we don't need it now, but can start thinking about how to use this to \n",
    "        # help with page navigation later on when scraping all pages.\n",
    "        pages = self.browser.find_element(By.XPATH, '/html/body/div[2]/div/div[2]/div[3]/div[2]/div/span[2]')\n",
    "        print(pages.get_attribute('innerHTML'))\n",
    "        \n",
    "        \n",
    "        # get page 2 by locating next page button and clickcking\n",
    "        self.browser.find_element(By.XPATH, '/html/body/div[2]/div/div[2]/div[3]/div[2]/div/span[1]/a/icon').click()\n",
    "        time.sleep(2)\n",
    "        page2savefile = self.bpath.prettypath / 'Page2Source.html'\n",
    "        self.save_page(page2savefile)\n",
    "        \n",
    "        # Display second results page URL\n",
    "        print(f'Results Page 2 URL: {self.browser.current_url}')\n",
    "        \n",
    "        # Get detail page for first company in list\n",
    "        self.browser.find_element(By.XPATH, '/html/body/div[2]/div/div[2]/table/tbody/tr[1]/td[1]/a').click()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Display detail page URL\n",
    "        print(f'Detail Page URL: {self.browser.current_url}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    PreviewSearchPage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see by the URL's that are gathered from the various pages that an API does exist. Although we don't have any documentation for this API (bacause it's not published)\n",
    "We can break down the fields to discover how to get all the pages for a city.\n",
    "\n",
    "Here's what was gathered:\n",
    "* Main Page URL: http://searchctbusiness.ctdata.org/\n",
    "* Results Page 1 URL: http://searchctbusiness.ctdata.org/search_results?query=Andover&index_field=place_of_business_city&sort_by=nm_name&sort_order=asc&page=1\n",
    "* 1 of 18\n",
    "* Results Page 2 URL: http://searchctbusiness.ctdata.org/search_results?page=2&start_date=1900-01-01&end_date=2019-04-01&query=Andover&index_field=place_of_business_city&sort_by=nm_name&sort_order=asc\n",
    "* Detail Page URL: http://searchctbusiness.ctdata.org/business/0589152\n",
    "\n",
    "The interesting thing here is the inconsistancy between page 1 and page 2, page 2 exposes much more of the API\n",
    "So if you break down the page 2 url, here are the components:\n",
    "\n",
    "* base url: http://searchctbusiness.ctdata.org/\n",
    "* page selector: search_results?page=2\n",
    "* &start_date=1900-01-01\n",
    "* &end_date=2019-04-01\n",
    "* &query=Andover\n",
    "* &index_field=place_of_business_city\n",
    "* &sort_by=nm_name\n",
    "* &sort_order=asc\n",
    "\n",
    "Pretty straight forward, can't get all options here but that's enough.\n",
    "Can try page 2 url modified for page 18 (last page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1450\"\n",
       "            height=\"900\"\n",
       "            src=\"http://searchctbusiness.ctdata.org/search_results?page=18&start_date=1900-01-01&end_date=2019-04-01&query=Andover&index_field=place_of_business_city&sort_by=nm_name&sort_order=asc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1f5c9e2908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='http://searchctbusiness.ctdata.org/search_results?page=18&start_date=1900-01-01&end_date=2019-04-01&query=Andover&index_field=place_of_business_city&sort_by=nm_name&sort_order=asc', width=1450, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And It works just fine.\n",
    "I also tried it with page 1 and it works.\n",
    "So now this can be used to scrap all pages using requests and beautifulsoup4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## ScrapeConnecticutBusiness.py\n",
    "\n",
    "The actual scraping Script.\n",
    "\n",
    "Note the module get_url uses the results created by PreviewSerchPage.py to create API access to Summary Page, this allows\n",
    "downlaoding all of the summary files using requests rather than selenium, which is much faster.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "### Setup\n",
    "\n",
    "* Create a list (self.citylist) with each entry being a sublist containing City Name, filename where results will be saved, and URL. This will be used to speed up\n",
    "  The feed for Concurrent Futures.\n",
    "* Set a counter to show progress (self.pages_to_download)\n",
    "* An empty list (self.pages_to_download) to save future results (number of summary pages for given city) in.\n",
    "\n",
    "### Initial scrape\n",
    "\n",
    "* Using (method: getnumpages) concurrent.futures.ProcessPoolExecutor (pure multiprocessing) fetch page1 for each city in self.citylist,\n",
    "\n",
    "  -  Each process calls parse method, passing city, filename, and url\n",
    "  -  Parse caches web pages when downloaded, the cache is checked for particular file, and file is loaded from cache if it exists. Otherwise, it is downloaded using requests.\n",
    "  -  The page is parsed using BeautifulSoup, extracting paginate-info. It finishes by returning city and numpages.\n",
    "  -  Use concurrent.futures.as_completed to capture results from parse (city and numpage)\n",
    "\n",
    "### Second to final scrape\n",
    "\n",
    "* After getting the first page of each city, it is necessary to use numpages to add entries to self.citylist so the remaining pages can be downloaded. This is the job of method add_new_pages.\n",
    "  Note that the method is called from a loop in the dispacth method, in groups separated by letter of the alphabet. This allows a break for the server in Connecticut, but mostly for diabolical reasons.\n",
    "\n",
    "  - It may seem as if this entire process could be done in the get_numpages method, but I intentionally prepair the list so that the multiprocessing part of the program doesn't contain any bottlenecks.\n",
    "  - Starting with page 2, create a new self.citylist for all remaining pages or each city.\n",
    "  - Call getnumpages for each group\n",
    "\n",
    "### Notes\n",
    "\n",
    "This entire process is blazingly fast, please note that max_workers should be changed for number of cores available in your CPU + 1. I add one, because for some reason, right or wrong, I get the feeling the script process will share one of the cores as a thread.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_url(self, city, page='1'):\n",
    "        return f'{self.bpath.base_url}search_results?page={page}' \\\n",
    "            f'&start_date=1900-01-01&end_date=2019-04-01&query={city.upper()}' \\\n",
    "            f'&index_field=place_of_business_city&sort_by=nm_name&sort_order=asc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScrapeConnecticut.py\n",
    "\n",
    "import BusinessPaths\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "class ScrapeConnecticut:\n",
    "    def __init__(self):\n",
    "        self.bpath = BusinessPaths.BusinessPaths()\n",
    "\n",
    "        self.citydict = {}\n",
    "        with self.bpath.cities_json.open() as fp:\n",
    "            self.citydict = json.load(fp)\n",
    "\n",
    "        self.citylist = []\n",
    "        for city, info in self.citydict['Connecticut'].items():\n",
    "            if city == 'Header':\n",
    "                continue\n",
    "            cityn = city.replace(' ', '')\n",
    "            self.citylist.append([city, self.bpath.htmlpath / f'{cityn}_page1.html', self.get_url(city)])\n",
    "        \n",
    "        self.pages_to_download = len(self.citylist)\n",
    "\n",
    "        self.numpages = []\n",
    "        # self.test_parse()\n",
    "        self.dispatch()\n",
    "        \n",
    "    def get_url(self, city, page='1'):\n",
    "        return f'{self.bpath.base_url}search_results?page={page}' \\\n",
    "            f'&start_date=1900-01-01&end_date=2019-04-01&query={city.upper()}' \\\n",
    "            f'&index_field=place_of_business_city&sort_by=nm_name&sort_order=asc'\n",
    "\n",
    "    def dispatch(self):\n",
    "        self.get_numpages()\n",
    "        for letter in string.ascii_uppercase:\n",
    "            self.add_new_pages(letter)\n",
    "            self.get_numpages()\n",
    "\n",
    "    def add_new_pages(self, letter):\n",
    "        self.citylist = []\n",
    "        findstart = True\n",
    "        for city, npages in self.numpages:\n",
    "            if findstart and not city.startswith(letter):\n",
    "                print(f'findstart city: {city}')\n",
    "                continue\n",
    "            findstart = False\n",
    "            if not city.startswith(letter):\n",
    "                print(f'for break, city: {city}')\n",
    "                break\n",
    "            # Remove spaces from city name.\n",
    "            cityn = city.replace(' ', '')\n",
    "\n",
    "            currentpage = 2\n",
    "            finalpage = int(npages)\n",
    "\n",
    "            while True:\n",
    "                # Check if done\n",
    "                if currentpage > finalpage:\n",
    "                    break\n",
    "                url = self.get_url(city, page=str(currentpage))\n",
    "\n",
    "                entry = [ city, self.bpath.htmlpath / f'{cityn}_page{currentpage}.html', url ]\n",
    "                self.citylist.append(entry)\n",
    "                currentpage += 1\n",
    "\n",
    "        self.pages_to_download = len(self.citylist)\n",
    "\n",
    "        # for debugging:\n",
    "        # filename = self.bpath.tmppath / 'citylist.text'\n",
    "        # with filename.open('w') as fp:\n",
    "        #     for entry in self.citylist:\n",
    "        #         fp.write(f'{entry}\\n')\n",
    "\n",
    "        print(f'Length citylist for {city}: {self.pages_to_download}')\n",
    "\n",
    "    def parse(self, city, filename, url):\n",
    "        # Will skip files already downloaded\n",
    "        # print(f'city: {city}, filename: {filename}, url: {url}')        \n",
    "        print(f'fetching {filename.name}')\n",
    "\n",
    "        if filename.exists():\n",
    "            with filename.open('rb') as fp:\n",
    "                page = fp.read()\n",
    "        else:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                time.sleep(.25)\n",
    "                page = response.content\n",
    "                with filename.open('wb') as fp:                \n",
    "                    fp.write(page)\n",
    "            else:\n",
    "                print(\"can;t download: {url}\")\n",
    "                return False\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        numpages = soup.find('span', {'class': \"paginate-info\"}).text.split()[2]\n",
    "        return city, numpages\n",
    "\n",
    "    def test_parse(self):\n",
    "        for city, filename, url in self.citylist:\n",
    "            city, numpages = self.parse(city, filename, url)\n",
    "            print(f'{city}: {numpages}')\n",
    "\n",
    "    def get_numpages(self):\n",
    "        countdown = self.pages_to_download\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "            city_pages = [ executor.submit(self.parse, city, filename, url) for city, filename, url in self.citylist ]\n",
    "            for future in concurrent.futures.as_completed(city_pages):\n",
    "                countdown -= 1\n",
    "                try:\n",
    "                    city, numpages = future.result()\n",
    "                    print(f'{city}: {numpages}')\n",
    "                except TypeError as exc:\n",
    "                    print(f'TypeError exception: {exc}')\n",
    "                else:\n",
    "                    self.numpages.append([city, numpages])\n",
    "                    print(f'Remaining files: {countdown}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ScrapeConnecticut()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## ExtractBusinessEntities.py\n",
    "\n",
    "This module extracts Business information from the files gathered by ScrapeConnecticut.py.\n",
    "It creates an intermediate dictionary which is saved as the JSON file /data/json/CompanyMaster.json\n",
    "Each cell of the dictionary contains the following information:\n",
    "\n",
    "* \"0667703\": {\n",
    "  - \"BusinessId\": \"0667703\",\n",
    "  - \"BusinessName\": \"119 SPENCER STREET, L.L.C.\",\n",
    "  - \"DetailUrl\": \"http://searchctbusiness.ctdata.org/business/0667703\",\n",
    "  - \"Filename\": \"../data/Idfiles/Id0667703.html\",\n",
    "  - \"DateFormed\": \"12/05/2000\",\n",
    "  - \"Status\": \"Active\",\n",
    "  - \"Address\": \"218 LAKE ROAD, ANDOVER, CT 06232\",\n",
    "  - \"CityState\": \"ANDOVER, CT\",\n",
    "  - \"PrincipalNames\": \"EDWARD A. HATEM\",\n",
    "  - \"AgentNames\": \"EDWARD A. HATEM\"\n",
    "  - }\n",
    "\n",
    "Key is BusinessId (also included in body for use later when creating SQL database).\n",
    "Other fields are self explainatory.\n",
    "\n",
    "### Setup\n",
    "\n",
    "* Create a summary file list (self.summary_file_list) which is just a file list form /data/html it uses 'page' from file name to identify proper files.\n",
    "\n",
    "### extract_company_info(self)\n",
    "\n",
    "* This method extracts the business data form each file using BeautifulSoup. For each file in summary_file_list:\n",
    "\n",
    "  - read file into variable page\n",
    "  - Convert to soup, using 'lxml' parser\n",
    "  - find tag 'table' which contains all of the business information\n",
    "  - find all 'th' tags which contain the header\n",
    "  - Replace various items in each header element (see self.header_replacepairs)\n",
    "  - save in self.header list\n",
    "  - Extract all 'tr' tags and pass to method strip_business\n",
    "\n",
    "### strip_business(self, trs)\n",
    "* continuation of extract_company_info pulls data from table tr tags\n",
    "\n",
    "  - for each element in trs:\n",
    "    - extract all td tags.\n",
    "    - if first td tag:\n",
    "      - there is a link to the detail page, extract that\n",
    "      - Extract the BusinessId from the URL\n",
    "      - Extract business name from link text\n",
    "      - create the filename where detail information will be saved\n",
    "      - save all this to the BusinessInfo dictionary\n",
    "    - Otherwise:\n",
    "      - extract td text, match to header and save to BusinessInfo dictionary\n",
    "\n",
    "### Save BusinessInfo dictionary as json file in data/jaon/CompanyMaster.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtractBusinessEntities.py\n",
    "\n",
    "import BusinessPaths\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import PrettifyPage\n",
    "import CreateDict\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "class ExtractBusinessEntities:\n",
    "    def __init__(self):\n",
    "        self.bpath = BusinessPaths.BusinessPaths()\n",
    "        self.pp = PrettifyPage.PrettifyPage()\n",
    "        self.cd = CreateDict.CreateDict()\n",
    "\n",
    "        self.summary_file_list = []\n",
    "        self.BusinessInfo = {}\n",
    "        self.city_node = {}\n",
    "        self.current_node = {}\n",
    "\n",
    "        self.header_replacepairs = [ ('(', ''), (')', ''), ('/', ''), (' ', ''),\n",
    "            ('MMDDYYYY', '') ]\n",
    "\n",
    "        self.header = []\n",
    "        self.create_business_info()\n",
    "\n",
    "    def create_business_info(self):\n",
    "        self.get_summary_file_list()\n",
    "        self.extract_company_info()\n",
    "        self.save_as_json()\n",
    "\n",
    "    # |++++++|+++++++++|+++++++++| Section 1 - Setup |+++++++++|+++++++++|+++++++++|\n",
    "\n",
    "    def get_summary_file_list(self):\n",
    "        path = self.bpath.htmlpath\n",
    "        self.summary_file_list = \\\n",
    "            [ filename for filename in path.iterdir() if filename.is_file() \\\n",
    "            and 'page' in filename.stem ]\n",
    "        self.summary_file_list.sort()\n",
    "\n",
    "    # |++++++|+++++++++|+++ Section 2 - Parse Summary Pages +++|+++++++++|+++++++++|\n",
    "\n",
    "    def extract_company_info(self):\n",
    "        self.header = []\n",
    "\n",
    "        for file in self.summary_file_list:            \n",
    "            print(f'Processing {file.name}')\n",
    "\n",
    "            city = str(file.stem).split('_')[0]\n",
    "            with file.open('rb') as fp:\n",
    "                page = fp.read()\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "            table = soup.find('table')\n",
    "            head = table.thead.find_all('th')\n",
    "\n",
    "            for element in head:\n",
    "                if not len(element):\n",
    "                    continue\n",
    "                item = element.text.strip()\n",
    "\n",
    "                for a, b in self.header_replacepairs:\n",
    "                    item = item.replace(a, b)\n",
    "                self.header.append(item)\n",
    "\n",
    "            trs = table.tbody.find_all('tr')\n",
    "            self.strip_business(trs)\n",
    "\n",
    "    def strip_business(self, trs):\n",
    "        base_url = 'http://searchctbusiness.ctdata.org'\n",
    "\n",
    "        for tr in trs:\n",
    "            tds = tr.find_all('td')\n",
    "\n",
    "            for n1, td in enumerate(tds):\n",
    "                if n1 == 0:\n",
    "                    detail_url = f\"{base_url}{td.a.get('href')}\"\n",
    "                    business_id = detail_url.split('/')[-1]                    \n",
    "                    business_name = td.a.text.strip()\n",
    "                    detail_filename = self.bpath.idpath / f'Id{business_id}.html'\n",
    "\n",
    "                    self.current_node = self.cd.add_node(self.BusinessInfo, business_id)\n",
    "                    self.cd.add_cell(self.current_node, 'BusinessId', business_id)\n",
    "                    self.cd.add_cell(self.current_node, 'BusinessName', business_name)\n",
    "                    self.cd.add_cell(self.current_node, 'DetailUrl', detail_url)\n",
    "                    self.cd.add_cell(self.current_node, 'Filename', os.fspath(detail_filename))\n",
    "                else:\n",
    "                    self.cd.add_cell(self.current_node, self.header[n1], \n",
    "                        td.text.strip())            \n",
    "\n",
    "    # |++++++|+++++++++| Section 3 - Get and Parse Detail Pages +++++++++|+++++++++|\n",
    "\n",
    "    def save_as_json(self):\n",
    "        with self.bpath.company_master_json.open('w') as fp:\n",
    "            json.dump(self.BusinessInfo, fp)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ExtractBusinessEntities()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## AddEntityDetail.py\n",
    "\n",
    "This module does all of the remaining scraping work. It uses the information that has already been stored in CompanyMaster.json to download and parse the detail company information.\n",
    "All if the files downloaded are sroted in the /data/Idfiles directory, and each bears the filename Idxxxxxxx.html where xxxxxxx is the BusinessId. There are close to 400,000 files in this group.\n",
    "\n",
    "The process uses concurrent.futures.ProcessPoolExecutor, since I started this project, download speed has improved greatly, at first I was getting times of 1.3 seconds per page, but at some point, I believe someone at the database location added an index that greatly increased the spped of download, now about .125 seconds or less per page.\n",
    "\n",
    "### method add_entity_detail(self)\n",
    "\n",
    "This method controls the entire process, from setup, download, parse and re-writing the updated CompanyMaster.json file.\n",
    "\n",
    "when done, each entry in the master will contain, the main information gathered by ExtractBusinessEntities.py, with the addition of additional company detail, principal information, agent information, and a list of all recorded filings.\n",
    "\n",
    "Here's what a typical entry looks like:\n",
    "\n",
    "* \"1169720\": {\n",
    "  - \"BusinessId\": \"1169720\",\n",
    "  - \"BusinessName\": \"182 WHEELING ROAD, LLC\",\n",
    "  - \"DetailUrl\": \"http://searchctbusiness.ctdata.org/business/1169720\",\n",
    "  - \"Filename\": \"../data/Idfiles/Id1169720.html\",\n",
    "  - \"DateFormed\": \"03/10/2015\",\n",
    "  - \"Status\": \"Active\",\n",
    "  - \"Address\": \"182 WHEELING ROAD, ANDOVER, CT 06232\",\n",
    "  - \"CityState\": \"ANDOVER, CT\",\n",
    "  - \"PrincipalNames\": \"HEATHER L. MORTIMER\",\n",
    "  - \"AgentNames\": \"GLENN T. TERK, ESQ.\",\n",
    "  - \"BusinessDetail\": {\n",
    "    - \"BusinessName\": \"182 WHEELING ROAD, LLC\",\n",
    "    - \"CitizenshipStateInc\": \"Domestic / CT\",\n",
    "    - \"LatestReport\": \"No Reports Found\",\n",
    "    - \"BusinessAddress\": \"182 WHEELING ROAD, ANDOVER, CT 06232\",\n",
    "    - \"BusinessType\": \"Unspecified\",\n",
    "    - \"MailingAddress\": \"C/O LAZ PARKING, 15 LEWIS STREET, HARTFORD, CT 06103\",\n",
    "    - \"BusinessStatus\": \"Active\",\n",
    "    - \"DateIncRegistration\": \"Mar 10, 2015\",\n",
    "    - \"Unused\": \"Unspecified\"\n",
    "  - },\n",
    "  - \"PrincipalsDetail\": {\n",
    "    - \"NameTitle\": \"HEATHER L. MORTIMER MANAGER\",\n",
    "    - \"BusinessAddress\": \"C/O LAZ PARKING, 15 LEWIS STREET, HARTFORD, CT 06103\",\n",
    "    - \"ResidenceAddress\": \"28 JONES STREET, AMSTON, CT 06231\"\n",
    "  - },\n",
    "  - \"AgentDetail\": {\n",
    "    - \"AgentName\": \"GLENN T. TERK, ESQ.\",\n",
    "    - \"AgentBusinessAddress\": \"15 LEWIS STREET, HARTFORD, CT 06103\",\n",
    "    - \"AgentResidenceAddress\": \"449 OLD RESERVOIR ROAD, WETHERSFIELD, CT 06109\"\n",
    "  - },\n",
    "  - \"FilingsDetail\": {\n",
    "    - \"1\": {}, This is a harmless software feature, so called because I didn't have enough time to find what causes it.\n",
    "    - \"2\": {\n",
    "      - \"FilingID\": \"0005294665\",\n",
    "      - \"FilingType\": \"ORGANIZATION\",\n",
    "      - \"DateofFiling\": \"03/10/2015\",\n",
    "      - \"VolumeType\": \"B\",\n",
    "      - \"Volume\": \"02044\",\n",
    "      - \"StartPage\": \"0685\",\n",
    "    - \"Pages\": \"2\"\n",
    "    - }\n",
    "  - }\n",
    "* }\n",
    "\n",
    "Parsing one of these files is different from the summary files. Each section is divided into a thead section and a tbody section\n",
    "The thead section only identifies what is to follow in the tbody section for example: <th class='table-name' colspan=\"4\">Business Details ... </th> and does not contain headers for the actual tbody content.\n",
    "Instead, the tbody section is made up of tr statements each contain pairs of td statements, odd one containing element name, and even element value.\n",
    "The filings section does not conform to this structure. Instead, it has a true heading contained in the thead statement, followed by a tbody statement containing one or more tr statements, each containing as many td statements as there are th statements in the thead section.\n",
    "\n",
    "There are 4 parts to each Business:\n",
    "* Business Details\n",
    "* Principals Details\n",
    "* Agent Summary\n",
    "* Filings Details\n",
    "\n",
    "### The process:\n",
    "***\n",
    "### Setup\n",
    "\n",
    "* create variables and lists used in __init__ method\n",
    "* load CustomerMaster.json into self.BusinessInfo dictionary\n",
    "* create detail download list (self.download_list) containing filename, url pairs for concurrent.futures download\n",
    "\n",
    "### Download\n",
    "\n",
    "method: download_detail() - calls fetch_url using all 4 cpu cores (change max_workers= #cores +1 for your processor)\n",
    "Files are cached, so this process can be interrupted and started again at a leater time without data loss.\n",
    "With about 400,000 pages for entire state, Total download time can be as long as 55.5 hours, though I have experienced much faster times in recent runs (I expect someone added an index to Db).\n",
    "\n",
    "### Parse detail files\n",
    "\n",
    "method: parse_detail()\n",
    "* For each file in self.filelist:\n",
    "  - Extract BusinessId from stem of filename.\n",
    "  - Read existing Business data from CompanyMaster (BusinessInfo dictionary)\n",
    "  - Read file and convert contents to soup\n",
    "  - Get node from BusinessInfo dictionary, if not found (shouldn't be the case, but possible if some files manually downloaded) add to mising link list/\n",
    "\n",
    "  - Now there are 4 sections of the file that need to be parsed, Business Details, Principal info agent info and filing info.\n",
    "    All are dispatched the same way:\n",
    "    \n",
    "    - try:\n",
    "        - discard = self.current_node['BusinessDetail']\n",
    "    - except KeyError:\n",
    "        - bnode = self.cd.add_node(self.current_node, 'BusinessDetail')\n",
    "        = self.parse_business_details(soup, bnode)\n",
    "\n",
    "  - This is trick code used to bypass already loaded nodes.\n",
    "    the try will fail if the node doesn't exist in the dictionary, causing a new node to be created, and then populated with a call to method parse_business_details(soup, bnode)\n",
    "    Note that the node is named differently for each section, bnode, pnode, anode, or fnode (1st letter same as 1st letter of section name (bnode = Business Details), \n",
    "    so when looking at code, you know which section of data you ar edealing with.\n",
    "  - After all files have been processed (400,000) for full boat, save missing list to /data/json (can be use later to retry for missing files).\n",
    "\n",
    "### parse_business_details(self, soup, bnode)\n",
    "* Parser for Business Details html\n",
    "  - Find table tag with id = 'detail-table'\n",
    "  - Find all 'tr' tags.\n",
    "  - for each tr tag:\n",
    "    - Find all 'td' tags These are arranged as pairs, title in odd td's, value in even td's.\n",
    "    - set pair flag to odd\n",
    "    - If an element is not present, there will be a pair of empty td cells  this is what the skipeven flag is used for, usually I will provide\n",
    "      empty node with proper title and value of unspecified.\n",
    "    - For each td:\n",
    "      - If odd:\n",
    "        - extract title, fills some empty tag conditions, and replaces special characters (that will give SQL inserts the fits).\n",
    "        - Set pair flag to even\n",
    "      - If even:\n",
    "        - if empty element, set value to 'Unspecified'\n",
    "        - There is a duplicate BusinessId tag in this section, bypass it when found as it is redundit.\n",
    "        - Add cell to dictionary\n",
    "        - set pair flag to odd\n",
    "\n",
    "### parse_principals(self, soup, pnode)\n",
    "* Parser for Principal information\n",
    "  - The html for principals is different from Business Details. Rather that use the odd, even td approach, it uses a header to get title information.\n",
    "  - Find table tag with id = 'principals'\n",
    "  - Find 'tr' tag in child node 'thead' (only one)\n",
    "    - find all 'th' tags\n",
    "      - for each, extract text, replace special characters and save in pheader list\n",
    "  - find all 'tr' tags in child node 'tbody' (many)\n",
    "    - for each 'tr' node:\n",
    "      - find all 'td' tags\n",
    "        - if 'td' element is empty, set value to 'Unspecified'\n",
    "        - Otherwise extract text value\n",
    "        - save cell to dictionary.\n",
    "\n",
    "### parse_agents(self, soup, anode)\n",
    "* parser for agent information\n",
    "  This is the same format as principals, only difference here is the 'id' of the table element\n",
    "  - Find table tag with id = 'agents'\n",
    "  - remainder same as for parse_principals method\n",
    "\n",
    "### parse_filings(self, soup, fnode)\n",
    "* Parser for company filings\n",
    "  This table is similar to principals and agents, with the exception that there may be None, or many.\n",
    "  To compensate for this, each filing will be given a separate node in the dictionary, with a generated sequence number as key.\n",
    "  - Find table tag with id = 'filings'\n",
    "  - Header extraction same as for principals and agents.\n",
    "  - tbody section has a separate tr for each filing, and contains the following elements:\n",
    "    - Filing ID\n",
    "    - Filing Type\n",
    "    - Date of Filing (MM/DD/YYYY)\n",
    "    - Volume Type\n",
    "    - Volume\n",
    "    - Start Page\n",
    "    - Pages #\n",
    "  - Parsing is the same as principals and agents with the addition of a sequence Node.\n",
    "  \n",
    "### Save results\n",
    "\n",
    "rewrite CustomerMaster.json file with new iformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AddEntityDetail.py\n",
    "\n",
    "import BusinessPaths\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import PrettifyPage\n",
    "import CreateDict\n",
    "import json\n",
    "import concurrent.futures\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# use Id0667703.html for testing, has all fields\n",
    "\n",
    "class AddEntityDetail:\n",
    "    def __init__(self):\n",
    "        self.bpath = BusinessPaths.BusinessPaths()\n",
    "        self.pp = PrettifyPage.PrettifyPage()\n",
    "        self.cd = CreateDict.CreateDict()\n",
    "\n",
    "        self.header_replacepairs = [ ('(', ''), (')', ''), ('/', ''), (' ', ''),\n",
    "            ('MMDDYYYY', '') ]\n",
    "\n",
    "        self.missing = []\n",
    "        self.BusinessInfo = {}\n",
    "        self.new_business_info = {}\n",
    "        self.current_node = {}\n",
    "        self.download_list = []\n",
    "        self.filelist = []\n",
    "        self.filecount = 0\n",
    "\n",
    "        self.add_entity_detail()\n",
    "    \n",
    "    def load_business_info(self):\n",
    "        with self.bpath.company_master_json.open() as fp:\n",
    "            self.BusinessInfo = json.load(fp)\n",
    "        \n",
    "        for BusinessId in self.BusinessInfo.keys():\n",
    "            url = self.BusinessInfo[BusinessId]['DetailUrl']\n",
    "            filename = Path(self.BusinessInfo[BusinessId]['Filename'])\n",
    "            self.download_list.append([filename, url])\n",
    "            self.filecount += 1\n",
    "        self.download_list.sort()\n",
    "        # self.cd.display_dict(self.BusinessInfo)\n",
    "\n",
    "    def add_entity_detail(self):        \n",
    "        self.load_business_info()\n",
    "        # self.download_detail()\n",
    "        self.parse_detail()\n",
    "        self.save_as_json()\n",
    "\n",
    "    def download_detail(self):\n",
    "        print('starting download')\n",
    "        countdown = self.filecount\n",
    "\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "            detail_info = [ executor.submit(self.fetch_url, filename, url) for filename, url in self.download_list if not filename.exists() ]\n",
    "\n",
    "            for future in concurrent.futures.as_completed(detail_info):\n",
    "                countdown -= 1\n",
    "                print(f'countdown: {countdown}')\n",
    "                filename = future.result()\n",
    "\n",
    "    def fetch_url(self, filename, url):\n",
    "        print(f'fetching {filename.name}')\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # time.sleep(.25)\n",
    "            with filename.open('wb') as fp:                \n",
    "                fp.write(response.content)\n",
    "        else:\n",
    "            print(f\"can't download: {url}\")\n",
    "            return False\n",
    "        return filename\n",
    "\n",
    "    def create_file_list(self):\n",
    "        self.filelist = [filename for filename in self.bpath.idpath.iterdir()\n",
    "            if filename.is_file and filename.stem.startswith('Id') ]\n",
    "        self.filelist.sort()\n",
    "\n",
    "    def parse_detail(self):\n",
    "        self.create_file_list()\n",
    "\n",
    "        for filename in self.filelist:\n",
    "            print(f'processing {filename.name}')\n",
    "\n",
    "            # Get existing dictionary node\n",
    "            business_id = filename.stem[2:]\n",
    "            try:\n",
    "                self.current_node = self.BusinessInfo[business_id]\n",
    "            except KeyError:\n",
    "                self.missing.append(os.fspath(filename))\n",
    "                continue\n",
    "\n",
    "            # read file\n",
    "            with filename.open() as fp:\n",
    "                page = fp.read()\n",
    "            soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "            # Add Business Detail\n",
    "            try:\n",
    "                discard = self.current_node['BusinessDetail']\n",
    "            except KeyError:\n",
    "                bnode = self.cd.add_node(self.current_node, 'BusinessDetail')\n",
    "                self.parse_business_details(soup, bnode)\n",
    "\n",
    "            # Add Principals Detail\n",
    "            try:\n",
    "                discard = self.current_node['PrincipalsDetail']\n",
    "            except KeyError:\n",
    "                pnode = self.cd.add_node(self.current_node, 'PrincipalsDetail')\n",
    "                self.parse_principals(soup, pnode)\n",
    "\n",
    "            # Add Agent Detail\n",
    "            try:\n",
    "                discard = self.current_node['AgentDetail']\n",
    "            except KeyError:\n",
    "                anode = self.cd.add_node(self.current_node, 'AgentDetail')\n",
    "                self.parse_agents(soup, anode)\n",
    "\n",
    "            # Add Filings Detail\n",
    "            try:\n",
    "                discard = self.current_node['FilingsDetail']\n",
    "            except KeyError:\n",
    "                fnode = self.cd.add_node(self.current_node, 'FilingsDetail')\n",
    "                self.parse_filings(soup, fnode)\n",
    "     \n",
    "        missingfiles = self.bpath.jsonpath / 'missing.json'\n",
    "        with missingfiles.open('w') as fp:\n",
    "            json.dump(self.missing, fp)\n",
    "            # verify\n",
    "            # self.cd.display_dict(self.current_node)\n",
    "            # print(self.current_node)\n",
    "\n",
    "    def parse_business_details(self, soup, bnode):\n",
    "        table = soup.find('table', {'class': 'detail-table'})\n",
    "        trs = table.tbody.find_all('tr')\n",
    "        for n, tr in enumerate(trs):            \n",
    "            tds = tr.find_all('td')            \n",
    "            odd = True\n",
    "            skipeven = False\n",
    "            for n1, td in enumerate(tds):\n",
    "                # print(f'\\n======================== tr_{n}, tr_ {n1} ========================')\n",
    "                # print(f'{self.pp.prettify(td, 2)}')\n",
    "                if skipeven:\n",
    "                    skipeven = False\n",
    "                    continue\n",
    "                if odd:\n",
    "                    if not len(td):\n",
    "                        if n == 2 and n1 == 2:\n",
    "                            title = 'BusinessType'\n",
    "                            value = 'Unspecified'\n",
    "                            self.cd.add_cell(bnode, title, value)\n",
    "                        elif n == 4 and n1 == 2:\n",
    "                            title = 'Unused'\n",
    "                            value = 'Unspecified'\n",
    "                            self.cd.add_cell(bnode, title, value)\n",
    "                        skipeven = True\n",
    "                        continue\n",
    "                    title = td.text.strip()\n",
    "                    if title[-1] == ':':\n",
    "                        title = title[:-1]\n",
    "                    title = title.replace('/', '')\n",
    "                    title = title.replace(' ', '')\n",
    "                    odd = False\n",
    "                else:\n",
    "                    if len(td):\n",
    "                        value = td.text.strip()\n",
    "                    else:\n",
    "                        value = 'Unspecified'\n",
    "                    # Already have business id, don't need twice\n",
    "                    if title == 'BusinessID':\n",
    "                        odd = True\n",
    "                        continue\n",
    "                    self.cd.add_cell(bnode, title, value)\n",
    "                    odd = True\n",
    "\n",
    "    def parse_principals(self, soup, pnode):\n",
    "        # Get header\n",
    "        principals = soup.find('table', {'id': 'principals'})\n",
    "        if principals:\n",
    "            phead = principals.thead.find('tr')\n",
    "            pheader = []\n",
    "            ths = phead.find_all('th')\n",
    "            for th in ths:\n",
    "                item = th.text.strip()\n",
    "                for a, b in self.header_replacepairs:\n",
    "                    item = item.replace(a, b)\n",
    "                pheader.append(item)\n",
    "\n",
    "            trs = principals.tbody.find_all('tr')\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td')\n",
    "                for n1, td in enumerate(tds):\n",
    "                    if len(td):\n",
    "                        self.cd.add_cell(pnode, pheader[n1], td.text.strip())\n",
    "                    else:\n",
    "                        self.cd.add_cell(pnode, pheader[n1], 'Unspecified')\n",
    "\n",
    "    def parse_agents(self, soup, anode):\n",
    "        agents = soup.find('table', {'id': 'agents'})\n",
    "        if agents:\n",
    "            aheader = []\n",
    "            ahead = agents.thead.find('tr')\n",
    "            ths = ahead.find_all('th')\n",
    "            for th in ths:\n",
    "                item = th.text.strip()\n",
    "                for a, b in self.header_replacepairs:\n",
    "                    item = item.replace(a, b)\n",
    "                aheader.append(item)\n",
    "\n",
    "            trs = agents.tbody.find_all('tr')\n",
    "            for tr in trs:\n",
    "                tds = tr.find_all('td')\n",
    "                for n1, td in enumerate(tds):\n",
    "                    if len(td):\n",
    "                        self.cd.add_cell(anode, aheader[n1], td.text.strip())\n",
    "                    else:\n",
    "                        self.cd.add_cell(anode, aheader[n1], 'Unspecified')\n",
    "\n",
    "    def parse_filings(self, soup, fnode):\n",
    "            filings = soup.find('table', {'id': 'filings'})\n",
    "            if filings:\n",
    "                fheader = []\n",
    "                fhead = filings.thead.find('tr')\n",
    "                ths = fhead.find_all('th')\n",
    "                for th in ths:\n",
    "                    title = th.text.strip()\n",
    "                    if '#' in title:\n",
    "                        title = title[:-2]\n",
    "                    for a, b in self.header_replacepairs:\n",
    "                        title = title.replace(a, b)\n",
    "                    fheader.append(title)\n",
    "                \n",
    "                trs = filings.find_all('tr')\n",
    "                seq = 1\n",
    "                for tr in trs:\n",
    "                    tds = tr.find_all('td')\n",
    "                    fitem = self.cd.add_node(fnode, str(seq))\n",
    "                    for n1, td in enumerate(tds):\n",
    "                        if len(td):\n",
    "                            self.cd.add_cell(fitem, fheader[n1], td.text.strip())\n",
    "                        else:\n",
    "                            self.cd.add_cell(fitem, fheader[n1], 'Unspecified')\n",
    "                    seq += 1\n",
    "    \n",
    "    def save_as_json(self):\n",
    "        with self.bpath.company_master_json.open('w') as fp:\n",
    "            json.dump(self.BusinessInfo, fp)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    AddEntityDetail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Database creation and load\n",
    "\n",
    "There are two modules involved\n",
    "Createtables.py and CreateDatabase.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### CreateTables.py\n",
    "\n",
    "This is a simple module that will create a new sqlite3 (can be simply modified for PostgreSQL, Oracle, Sybase or others)\n",
    "It also builds Insert Queries when passed table name and a list of values (from called module)\n",
    "\n",
    "Explain on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateTables.py\n",
    "\n",
    "import BusinessPaths\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "\n",
    "class CreateTables:\n",
    "    def __init__(self):\n",
    "        self.bpath = BusinessPaths.BusinessPaths()\n",
    "\n",
    "        self.insert_statements = {}\n",
    "        self.dbcon = None\n",
    "        self.dbcur = None\n",
    "\n",
    "    def db_connect(self):\n",
    "        try:\n",
    "            self.dbcon = sqlite3.connect(self.bpath.CompanyMasterDb)\n",
    "            self.dbcur = self.dbcon.cursor()\n",
    "        except sqlite3.Error as e:\n",
    "            print(e)\n",
    "\n",
    "    def create_tables(self):\n",
    "        self.db_connect()\n",
    "        company = [ \n",
    "            'BusinessId', 'BusinessName', 'DetailURL', 'Filename', 'DateFormed',\n",
    "            'Status', 'Address', 'CityState', 'PrincipalNames', 'AgentNames'\n",
    "        ] \n",
    "        self.insert_statements['Company'] = self.create_table(company, 'Company')\n",
    "\n",
    "        details = [\n",
    "            'BusinessId', 'BusinessName', 'CitizenshipStateInc', 'LatestReport',\n",
    "            'BusinessAddress', 'BusinessType', 'MailingAddress', 'BusinessStatus',\n",
    "            'DateIncRegistration', 'Unused'\n",
    "        ]\n",
    "        self.insert_statements['Details'] = self.create_table(\n",
    "            details, 'Details')\n",
    "\n",
    "        # need to assign principalId\n",
    "        principals = [\n",
    "            'BusinessId', 'NameTitle', 'BusinessAddress', 'ResidenceAddress'\n",
    "        ]\n",
    "        self.insert_statements['Principals'] = self.create_table(\n",
    "            principals, 'Principals')\n",
    "\n",
    "        agents = [\n",
    "            'BusinessId', 'AgentName', 'AgentBusinessAddress',\n",
    "            'AgentResidenceAddress'\n",
    "        ]\n",
    "        self.insert_statements['Agents'] = self.create_table(\n",
    "            agents, 'Agents')\n",
    "\n",
    "        filings = [\n",
    "            'BusinessId', 'SeqNo', 'FilingId', 'FilingType', 'DateofFiling',\n",
    "            'VolumeType', 'Volume', 'StartPage', 'Pages'\n",
    "        ]\n",
    "        self.insert_statements['Filings'] = self.create_table(\n",
    "            filings, 'Filings')\n",
    "\n",
    "    def create_table(self, header, tablename='tname'):\n",
    "        \"\"\"\n",
    "        Create CorpTable from header record of self.bpath.corporation_master\n",
    "        \"\"\"\n",
    "        qmarks = (f\"?, \" * len(header))[:-2]\n",
    "        base_insert = f\"INSERT INTO {tablename} VALUES \"\n",
    "        columns = ', '.join(header)\n",
    "        sqlstr = f'DROP TABLE IF EXISTS {tablename};'\n",
    "        self.dbcur.execute(sqlstr)\n",
    "        sqlstr = f'CREATE TABLE IF NOT EXISTS {tablename} ({columns});'\n",
    "        # print(sqlstr)\n",
    "        self.dbcur.execute(sqlstr)\n",
    "        self.db_commit()\n",
    "        return base_insert\n",
    "\n",
    "    def insert_data(self, tablename, columns):\n",
    "        # print(f'\\n{tablename}: {columns}')\n",
    "        dbcolumns = None\n",
    "        try:\n",
    "            dbcolumns = '('\n",
    "            for item in columns:\n",
    "                dbcolumns = f\"{dbcolumns}'{item}', \"\n",
    "            dbcolumns = f\"{dbcolumns[:-2]});\"\n",
    "            sqlstr = f'{self.insert_statements[tablename]}{dbcolumns}'\n",
    "            # print(f'\\n{tablename}: {sqlstr}')\n",
    "            self.dbcur.execute(sqlstr)\n",
    "        except sqlite3.OperationalError:\n",
    "            print(f'OperationalError:\\n{sqlstr}')\n",
    "            sys.exit(0)\n",
    "        except sqlite3.IntegrityError:\n",
    "            print(f'IntegrityError:\\n{sqlstr}')\n",
    "            sys.exit(0)\n",
    "\n",
    "\n",
    "    def db_close(self, rollback=False):\n",
    "        if rollback:\n",
    "            self.dbcon.rollback()\n",
    "        else:\n",
    "            self.dbcon.commit()\n",
    "        self.dbcon.close()\n",
    "\n",
    "    def db_commit(self):\n",
    "        self.dbcon.commit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ct = CreateTables()\n",
    "    ct.create_tables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### CreateDatabase.py\n",
    "\n",
    "This module creates the CompanyMaster.db Database in /data/database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CreateDatabase.py\n",
    "\n",
    "import BusinessPaths\n",
    "import CreateTables\n",
    "import sqlite3\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "class CreateDatabase:\n",
    "    def __init__(self):\n",
    "        self.bpath = BusinessPaths.BusinessPaths()\n",
    "        self.cretab = CreateTables.CreateTables()\n",
    "\n",
    "        with self.bpath.company_master_json.open() as fp:\n",
    "            self.master = json.load(fp)\n",
    "\n",
    "        self.dbcon = None\n",
    "        self.dbcur = None\n",
    "\n",
    "        self.CompanyMain = {}\n",
    "        self.Detail = {}\n",
    "        self.Principals = {}\n",
    "        self.Agents = {}\n",
    "        self.Filings = {}\n",
    "\n",
    "        self.insert_statements = {}\n",
    "\n",
    "        self.db_connect()\n",
    "        self.makedb()\n",
    "        self.db_close()\n",
    "\n",
    "    def db_connect(self):\n",
    "        try:\n",
    "            self.dbcon = sqlite3.connect(self.bpath.CompanyMasterDb)\n",
    "            self.dbcur = self.dbcon.cursor()\n",
    "        except sqlite3.Error as e:\n",
    "            print(e)\n",
    "\n",
    "    def db_close(self, rollback=False):\n",
    "        if rollback:\n",
    "            self.dbcon.rollback()\n",
    "        else:\n",
    "            self.dbcon.commit()\n",
    "        self.dbcon.close()\n",
    "\n",
    "    def db_commit(self):\n",
    "        self.dbcon.commit()\n",
    "\n",
    "    def insert_business_id(self, business_id, oldvalue):\n",
    "        newvalue = {}\n",
    "        newvalue['BusinessId'] = business_id\n",
    "        for key, dvalue in oldvalue.items():\n",
    "            newvalue[key] = dvalue\n",
    "        return newvalue\n",
    "\n",
    "    def split_dict(self):\n",
    "        if not self.bpath.company_main.exists():\n",
    "            for business_id, details in self.master.items():\n",
    "                firstcompany = True\n",
    "                print(f'Business_id: {business_id}')\n",
    "                for key, value in details.items():\n",
    "                    if key == 'BusinessDetail':\n",
    "                        if len(value):\n",
    "                            newvalue = self.insert_business_id(business_id, value)\n",
    "                            self.Detail[business_id] = newvalue\n",
    "                    elif key == 'PrincipalsDetail':\n",
    "                        if len(value):\n",
    "                            newvalue = self.insert_business_id(business_id, value)\n",
    "                            self.Principals[business_id] = newvalue\n",
    "                    elif key == 'AgentDetail':\n",
    "                        if len(value):\n",
    "                            newvalue = self.insert_business_id(business_id, value)\n",
    "                            self.Agents[business_id] = newvalue\n",
    "                    elif key == 'FilingsDetail':\n",
    "                        if len(value):\n",
    "                            newvalue = self.insert_business_id(business_id, value)\n",
    "                            self.Filings[business_id] = newvalue\n",
    "                    else:\n",
    "                        if firstcompany:\n",
    "                            cnode = self.CompanyMain[business_id] = {}\n",
    "                            firstcompany = False\n",
    "                        cnode[key] = value\n",
    "            self.save_split_files()\n",
    "        else:\n",
    "            self.load_split_files()\n",
    "    \n",
    "    def save_split_files(self):\n",
    "        with self.bpath.company_main.open('w') as fp:\n",
    "            json.dump(self.CompanyMain, fp)\n",
    "\n",
    "        with self.bpath.company_detail.open('w') as fp:\n",
    "            json.dump(self.Detail, fp)\n",
    "\n",
    "        with self.bpath.company_principals.open('w') as fp:\n",
    "            json.dump(self.Principals, fp)\n",
    "\n",
    "        with self.bpath.company_agents.open('w') as fp:\n",
    "            json.dump(self.Agents, fp)\n",
    "\n",
    "        with self.bpath.company_filings.open('w') as fp:\n",
    "            json.dump(self.Filings, fp)\n",
    "\n",
    "    def load_split_files(self):\n",
    "        with self.bpath.company_main.open() as fp:\n",
    "            self.CompanyMain = json.load(fp)\n",
    "\n",
    "        with self.bpath.company_detail.open() as fp:\n",
    "            self.Detail = json.load(fp)\n",
    "\n",
    "        with self.bpath.company_principals.open() as fp:\n",
    "            self.Principals = json.load(fp)\n",
    "\n",
    "        with self.bpath.company_agents.open() as fp:\n",
    "            self.Agents = json.load(fp)\n",
    "\n",
    "        with self.bpath.company_filings.open() as fp:\n",
    "            self.Filings = json.load(fp)\n",
    "\n",
    "    def makedb(self):\n",
    "        self.cretab.create_tables()\n",
    "        self.split_dict()\n",
    "\n",
    "        self.insert_data(self.CompanyMain, 'Company')\n",
    "        self.insert_data(self.Detail, 'Details')\n",
    "        self.insert_data(self.Principals, 'Principals')\n",
    "        self.insert_data(self.Agents, 'Agents')\n",
    "        self.insert_data(self.Filings, 'Filings')\n",
    "\n",
    "    def insert_data(self, data_dict, tablename):\n",
    "        print(f'Loading {tablename} table')\n",
    "        self.base_insert = f\"INSERT INTO {tablename} VALUES \"        \n",
    "        keys = list(data_dict.keys())\n",
    "        for key in keys:\n",
    "            try:\n",
    "                data = data_dict[key]\n",
    "                # print(f'data: {data}')\n",
    "                columns = f\"(\"\n",
    "                for item in data:\n",
    "                    value = data_dict[key][item]\n",
    "                    if not len(value):\n",
    "                        continue\n",
    "                    if isinstance(value, dict):\n",
    "                        columns = f\"{columns}'{item}', \"\n",
    "                        for key1, subitem in value.items():\n",
    "                            subitem = subitem.replace(\"'\", \"''\")\n",
    "                            columns = f\"{columns}'{subitem}', \"\n",
    "                        break\n",
    "                    value = value.replace(\"'\", \"''\")\n",
    "                    columns = f\"{columns}'{value}', \"\n",
    "                columns = f\"{columns[:-2]});\"\n",
    "                sqlstr = f'{self.base_insert}{columns}'\n",
    "                # print(f'sqlstr: {sqlstr}')\n",
    "                self.dbcon.execute(sqlstr)\n",
    "            except sqlite3.OperationalError:\n",
    "                print(f'sqlite3.OperationalError\\ndata: {data}\\nsqlstr: {sqlstr}')\n",
    "                sys.exit(0)\n",
    "            except AttributeError:\n",
    "                print(f'AttributeError\\nkey: {key}, item: {item}, value: {value}, data: {data}')\n",
    "                sys.exit(0)\n",
    "        self.db_commit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    CreateDatabase()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to backup data including all 18 thousand summary pages and 400 thousand detail pages:\n",
    "* Use rsync which is a quantum leap over cp in speed (FYI: includes using over network)\n",
    "* get a list: \n",
    "  - find ./Idfiles/ -name *.html > ./tmp/Idlist.txt\n",
    "  - run from makerProjectApril2019 directory\n",
    "    - rsync -avz ./data/ backup_directory/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
